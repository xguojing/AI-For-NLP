{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why we need machine learning methods instead of creating a complicated formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习解决复杂问题的能力是传统方法无法比拟的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Wha't's the disadvantages of `the 1st Random Choosen` methods in our course? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机选择就好比是对结果的胡乱猜想，运气好的话能够得到一个较好的答案，运气不好的话永远也得不到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Is the `2nd method supervised direction` better than 1st one?  What's the disadvantages of `the 2nd supversied directin` method? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种方法是第一种方法的一个改进，是有方向的猜想，当我们在一个方向上能够得到更好的结果时则继续往这个方向走，反之则更换方向。可以避免在错误的方向上越走越远\n",
    "\n",
    "**缺点：**当参数空间变大时，方向空间呈指数增长，这时我们就无法使用方向监督的方法对函数进行拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why do we use `Derivative / Gredient` to fit a target function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的目标是求损失函数的最小值，沿着梯度的负方向也就是损失函数下降最快的方向更新参数值，能够很快的得到一个局部最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. In the words 'Gredient Descent', what's the `Gredient` and what's the `Descent`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gredient：函数在某一点的梯度是一个自变量空间内的向量。自变量顺着梯度方向变化时函数值上升得最快。梯度的模（长度）是函数值上升的速率。梯度朝某方向投影的长度是自变量顺着该方向变化时函数值的变化率。对于可微的函数 $f(x,y,z)$， $\\left( \\frac { \\partial f } { \\partial x } , \\frac { \\partial f } { \\partial y } , \\frac { \\partial f } { \\partial z } \\right)$ 为 $f$ 的梯度。\n",
    "\n",
    "- Descent：一种优化算法，该算法从任一点开始，沿该点梯度的反方向运动一段距离，再沿新位置的梯度反方向运行一段距离 ...... 如此迭代。解一直朝下坡最陡的方向运动，希望能运动到函数的全局最小点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What's the advantages of `the 3rd gradient descent method` compared to the previous methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**优点：**\n",
    "\n",
    "- 对比损失函数图像，梯度下降法的曲线更平滑。\n",
    "- 梯度下降法能够使模型更快收敛，且loss值更小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using the simple words to describe: What's the machine leanring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过计算的手段，利用经验来改善系统自身的性能。在计算机系统中，“经验”通常以“数据”形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”的算法，即“学习算法”。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
